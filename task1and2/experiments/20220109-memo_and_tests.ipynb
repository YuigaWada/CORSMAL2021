{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2022/01/09 Memo & Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aim\n",
    "remember many forgetten architectures of torch and/or others.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get & Add the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "current_dir: Path = pathlib.Path().cwd().resolve()\n",
    "project_root: Path = current_dir.parent\n",
    "data_dir: Path = project_root / \"data\"\n",
    "\n",
    "sys.path.append(str(project_root))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 8, 128, 1500])\n",
      "torch.Size([5, 16, 128, 1500])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "groups = 1\n",
    "batches = 5\n",
    "in_channels = 8\n",
    "out_channels = 16\n",
    "freq_axis = 128\n",
    "time_axis = 1500\n",
    "filters = torch.randn(out_channels, in_channels // groups, 3, 3)\n",
    "inputs = torch.randn(batches, in_channels, freq_axis, time_axis)\n",
    "outputs = F.conv2d(inputs, filters, padding=1, stride=(1, 1))\n",
    "\n",
    "print(inputs.size())\n",
    "print(outputs.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 8, 128, 1500])\n",
      "torch.Size([5, 48, 128, 375])\n",
      "torch.Size([5, 96, 128, 375])\n"
     ]
    }
   ],
   "source": [
    "from corsmal_challenge.models.convolution import DepthWiseConv2d, PointWiseConv2d\n",
    "\n",
    "\n",
    "batches = 5\n",
    "in_channels = 8\n",
    "expansion = 6\n",
    "freq_axis = 128\n",
    "time_axis = 1500\n",
    "\n",
    "inputs = torch.randn(batches, in_channels, freq_axis, time_axis)\n",
    "\n",
    "\n",
    "dconv = DepthWiseConv2d(\n",
    "    in_channels,\n",
    "    expansion,\n",
    "    kernel_size=(5, 5),\n",
    "    stride=(1, 5 - 1),\n",
    "    bias=True,\n",
    "    padding=2,\n",
    ")\n",
    "pconv = PointWiseConv2d(\n",
    "    dconv.out_channels,\n",
    "    dconv.out_channels * 2,\n",
    "    bias=False,\n",
    ")\n",
    "\n",
    "\n",
    "outputs: torch.Tensor = dconv(inputs)\n",
    "outputs2: torch.Tensor = pconv(outputs)\n",
    "\n",
    "print(inputs.size())\n",
    "print(outputs.size())\n",
    "print(outputs2.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 8, 128, 1500])\n",
      "torch.Size([5, 8, 128, 1500])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "InvertedResBlock                         --\n",
       "├─BatchNorm2d: 1-1                       16\n",
       "├─PointWiseConv2d: 1-2                   384\n",
       "├─BatchNorm2d: 1-3                       96\n",
       "├─DepthWiseConv2d: 1-4                   144\n",
       "├─BatchNorm2d: 1-5                       96\n",
       "├─PointWiseConv2d: 1-6                   392\n",
       "=================================================================\n",
       "Total params: 1,128\n",
       "Trainable params: 1,128\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchinfo\n",
    "from corsmal_challenge.models.convolution import InvertedResBlock\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batches = 5\n",
    "in_channels = 8\n",
    "expansion = 6\n",
    "freq_axis = 128\n",
    "time_axis = 1500\n",
    "\n",
    "inputs = torch.randn(batches, in_channels, freq_axis, time_axis).to(device)\n",
    "\n",
    "irb = InvertedResBlock(\n",
    "    in_channels,\n",
    "    bias=True,\n",
    "    kernel_size=(3, 1),\n",
    "    expansion=6,\n",
    ")\n",
    "irb2 = InvertedResBlock(\n",
    "    in_channels,\n",
    "    bias=True,\n",
    "    kernel_size=(3, 1),\n",
    "    expansion=6,\n",
    ")\n",
    "\n",
    "irb = irb.to(device)\n",
    "irb2 = irb2.to(device)\n",
    "\n",
    "outputs: torch.Tensor = irb2(irb(inputs))\n",
    "\n",
    "print(inputs.size())\n",
    "print(outputs.size())\n",
    "\n",
    "torchinfo.summary(irb2, inputs_size=(batches, in_channels, freq_axis, time_axis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 8, 128, 1500])\n",
      "torch.Size([8, 1, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "LightCNNEncoder                          --\n",
       "├─SquaredReLU: 1-1                       --\n",
       "├─BatchNorm2d: 1-2                       16\n",
       "├─DepthWiseConv2d: 1-3                   72\n",
       "├─BatchNorm2d: 1-4                       16\n",
       "├─PointWiseConv2d: 1-5                   144\n",
       "├─Sequential: 1-6                        --\n",
       "│    └─InvertedResBlock: 2-1             --\n",
       "│    │    └─BatchNorm2d: 3-1             32\n",
       "│    │    └─PointWiseConv2d: 3-2         1,536\n",
       "│    │    └─BatchNorm2d: 3-3             192\n",
       "│    │    └─DepthWiseConv2d: 3-4         864\n",
       "│    │    └─BatchNorm2d: 3-5             192\n",
       "│    │    └─PointWiseConv2d: 3-6         1,536\n",
       "│    └─InvertedResBlock: 2-2             --\n",
       "│    │    └─BatchNorm2d: 3-7             32\n",
       "│    │    └─PointWiseConv2d: 3-8         1,536\n",
       "│    │    └─BatchNorm2d: 3-9             192\n",
       "│    │    └─DepthWiseConv2d: 3-10        864\n",
       "│    │    └─BatchNorm2d: 3-11            192\n",
       "│    │    └─PointWiseConv2d: 3-12        1,536\n",
       "│    └─InvertedResBlock: 2-3             --\n",
       "│    │    └─BatchNorm2d: 3-13            32\n",
       "│    │    └─PointWiseConv2d: 3-14        1,536\n",
       "│    │    └─BatchNorm2d: 3-15            192\n",
       "│    │    └─DepthWiseConv2d: 3-16        864\n",
       "│    │    └─BatchNorm2d: 3-17            192\n",
       "│    │    └─PointWiseConv2d: 3-18        1,536\n",
       "│    └─InvertedResBlock: 2-4             --\n",
       "│    │    └─BatchNorm2d: 3-19            32\n",
       "│    │    └─PointWiseConv2d: 3-20        1,536\n",
       "│    │    └─BatchNorm2d: 3-21            192\n",
       "│    │    └─DepthWiseConv2d: 3-22        864\n",
       "│    │    └─BatchNorm2d: 3-23            192\n",
       "│    │    └─PointWiseConv2d: 3-24        1,536\n",
       "│    └─InvertedResBlock: 2-5             --\n",
       "│    │    └─BatchNorm2d: 3-25            32\n",
       "│    │    └─PointWiseConv2d: 3-26        1,536\n",
       "│    │    └─BatchNorm2d: 3-27            192\n",
       "│    │    └─DepthWiseConv2d: 3-28        864\n",
       "│    │    └─BatchNorm2d: 3-29            192\n",
       "│    │    └─PointWiseConv2d: 3-30        1,536\n",
       "│    └─InvertedResBlock: 2-6             --\n",
       "│    │    └─BatchNorm2d: 3-31            32\n",
       "│    │    └─PointWiseConv2d: 3-32        1,536\n",
       "│    │    └─BatchNorm2d: 3-33            192\n",
       "│    │    └─DepthWiseConv2d: 3-34        864\n",
       "│    │    └─BatchNorm2d: 3-35            192\n",
       "│    │    └─PointWiseConv2d: 3-36        1,536\n",
       "├─BatchNorm2d: 1-7                       32\n",
       "├─DepthWiseConv2d: 1-8                   144\n",
       "├─BatchNorm2d: 1-9                       32\n",
       "├─PointWiseConv2d: 1-10                  1,088\n",
       "├─Sequential: 1-11                       --\n",
       "│    └─InvertedResBlock: 2-7             --\n",
       "│    │    └─BatchNorm2d: 3-37            128\n",
       "│    │    └─PointWiseConv2d: 3-38        24,576\n",
       "│    │    └─BatchNorm2d: 3-39            768\n",
       "│    │    └─DepthWiseConv2d: 3-40        3,456\n",
       "│    │    └─BatchNorm2d: 3-41            768\n",
       "│    │    └─PointWiseConv2d: 3-42        24,576\n",
       "│    └─InvertedResBlock: 2-8             --\n",
       "│    │    └─BatchNorm2d: 3-43            128\n",
       "│    │    └─PointWiseConv2d: 3-44        24,576\n",
       "│    │    └─BatchNorm2d: 3-45            768\n",
       "│    │    └─DepthWiseConv2d: 3-46        3,456\n",
       "│    │    └─BatchNorm2d: 3-47            768\n",
       "│    │    └─PointWiseConv2d: 3-48        24,576\n",
       "│    └─InvertedResBlock: 2-9             --\n",
       "│    │    └─BatchNorm2d: 3-49            128\n",
       "│    │    └─PointWiseConv2d: 3-50        24,576\n",
       "│    │    └─BatchNorm2d: 3-51            768\n",
       "│    │    └─DepthWiseConv2d: 3-52        3,456\n",
       "│    │    └─BatchNorm2d: 3-53            768\n",
       "│    │    └─PointWiseConv2d: 3-54        24,576\n",
       "│    └─InvertedResBlock: 2-10            --\n",
       "│    │    └─BatchNorm2d: 3-55            128\n",
       "│    │    └─PointWiseConv2d: 3-56        24,576\n",
       "│    │    └─BatchNorm2d: 3-57            768\n",
       "│    │    └─DepthWiseConv2d: 3-58        3,456\n",
       "│    │    └─BatchNorm2d: 3-59            768\n",
       "│    │    └─PointWiseConv2d: 3-60        24,576\n",
       "│    └─InvertedResBlock: 2-11            --\n",
       "│    │    └─BatchNorm2d: 3-61            128\n",
       "│    │    └─PointWiseConv2d: 3-62        24,576\n",
       "│    │    └─BatchNorm2d: 3-63            768\n",
       "│    │    └─DepthWiseConv2d: 3-64        3,456\n",
       "│    │    └─BatchNorm2d: 3-65            768\n",
       "│    │    └─PointWiseConv2d: 3-66        24,576\n",
       "│    └─InvertedResBlock: 2-12            --\n",
       "│    │    └─BatchNorm2d: 3-67            128\n",
       "│    │    └─PointWiseConv2d: 3-68        24,576\n",
       "│    │    └─BatchNorm2d: 3-69            768\n",
       "│    │    └─DepthWiseConv2d: 3-70        3,456\n",
       "│    │    └─BatchNorm2d: 3-71            768\n",
       "│    │    └─PointWiseConv2d: 3-72        24,576\n",
       "├─BatchNorm2d: 1-12                      128\n",
       "├─DepthWiseConv2d: 1-13                  576\n",
       "├─BatchNorm2d: 1-14                      128\n",
       "├─PointWiseConv2d: 1-15                  16,640\n",
       "=================================================================\n",
       "Total params: 370,760\n",
       "Trainable params: 370,760\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchinfo\n",
    "\n",
    "from corsmal_challenge.models.convolution import LightCNNEncoder\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batches = 8\n",
    "in_channels = 8\n",
    "expansion = 6\n",
    "freq_axis = 128\n",
    "time_axis = 1500\n",
    "# time_axis = 512\n",
    "\n",
    "inputs = torch.randn(batches, in_channels, freq_axis, time_axis)\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "encoder = LightCNNEncoder(in_channels=in_channels)\n",
    "encoder = encoder.to(device)\n",
    "\n",
    "outputs: torch.Tensor = encoder(inputs)\n",
    "\n",
    "print(inputs.size())\n",
    "print(outputs.size())\n",
    "\n",
    "torchinfo.summary(encoder, inputs_size=(batches, in_channels, freq_axis, time_axis))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.size(): torch.Size([7, 1000, 144])\n",
      "b.size(): torch.Size([7, 1000, 144])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "MultiheadedSelfAttention                 --\n",
       "├─Linear: 1-1                            62,640\n",
       "├─Dropout: 1-2                           --\n",
       "├─Linear: 1-3                            20,880\n",
       "├─Dropout: 1-4                           --\n",
       "=================================================================\n",
       "Total params: 83,520\n",
       "Trainable params: 83,520\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from corsmal_challenge.models.transformer import MultiheadedSelfAttention\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batches = 7\n",
    "seq_len = 1000\n",
    "embed_dim = 144\n",
    "num_heads = 12\n",
    "\n",
    "\n",
    "a = torch.randn(batches, seq_len, embed_dim)\n",
    "mha = MultiheadedSelfAttention(embed_dim, num_heads)\n",
    "a = a.to(device)\n",
    "mha = mha.to(device)\n",
    "\n",
    "\n",
    "b: torch.Tensor = mha(a)\n",
    "print(f\"a.size(): {a.size()}\")\n",
    "print(f\"b.size(): {b.size()}\")\n",
    "\n",
    "torchinfo.summary(mha, inputs_size=(batches, seq_len, embed_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.size(): torch.Size([7, 1000, 144])\n",
      "b.size(): torch.Size([7, 1000, 144])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "PositionalEmbedding                      --\n",
       "=================================================================\n",
       "Total params: 0\n",
       "Trainable params: 0\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim,\n",
    "        max_len=8000,\n",
    "        freq=16000.0,\n",
    "    ):\n",
    "        import math\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(freq) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div)\n",
    "        pe[:, 1::2] = torch.cos(position * div)\n",
    "        pe: torch.Tensor = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.pe[: x.size(0), :]\n",
    "        return x\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batches = 7\n",
    "seq_len = 1000\n",
    "embed_dim = 144\n",
    "num_heads = 12\n",
    "a = torch.randn(batches, seq_len, embed_dim).to(device)\n",
    "pose = PositionalEmbedding(embed_dim).to(device)\n",
    "\n",
    "b = pose(a)\n",
    "\n",
    "\n",
    "print(f\"a.size(): {a.size()}\")\n",
    "print(f\"b.size(): {b.size()}\")\n",
    "\n",
    "torchinfo.summary(pose, inputs_size=(batches, seq_len, embed_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.size(): torch.Size([7, 1000, 128])\n",
      "b.size(): torch.Size([7, 1001, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===========================================================================\n",
       "Layer (type:depth-idx)                             Param #\n",
       "===========================================================================\n",
       "TransformerEncoder                                 --\n",
       "├─PositionalEncoding: 1-1                          --\n",
       "├─MultiheadedSelfAttention: 1-2                    --\n",
       "│    └─Linear: 2-1                                 49,536\n",
       "│    └─Dropout: 2-2                                --\n",
       "│    └─Linear: 2-3                                 16,512\n",
       "│    └─Dropout: 2-4                                --\n",
       "├─Sequential: 1-3                                  --\n",
       "│    └─TransformerEncoderBlock: 2-5                --\n",
       "│    │    └─LayerNorm: 3-1                         256\n",
       "│    │    └─MultiheadedSelfAttention: 3-2          66,048\n",
       "│    │    └─FFN: 3-3                               65,920\n",
       "│    └─TransformerEncoderBlock: 2-6                --\n",
       "│    │    └─LayerNorm: 3-4                         256\n",
       "│    │    └─MultiheadedSelfAttention: 3-5          66,048\n",
       "│    │    └─FFN: 3-6                               65,920\n",
       "│    └─TransformerEncoderBlock: 2-7                --\n",
       "│    │    └─LayerNorm: 3-7                         256\n",
       "│    │    └─MultiheadedSelfAttention: 3-8          66,048\n",
       "│    │    └─FFN: 3-9                               65,920\n",
       "│    └─TransformerEncoderBlock: 2-8                --\n",
       "│    │    └─LayerNorm: 3-10                        256\n",
       "│    │    └─MultiheadedSelfAttention: 3-11         66,048\n",
       "│    │    └─FFN: 3-12                              65,920\n",
       "│    └─TransformerEncoderBlock: 2-9                --\n",
       "│    │    └─LayerNorm: 3-13                        256\n",
       "│    │    └─MultiheadedSelfAttention: 3-14         66,048\n",
       "│    │    └─FFN: 3-15                              65,920\n",
       "===========================================================================\n",
       "Total params: 727,168\n",
       "Trainable params: 727,168\n",
       "Non-trainable params: 0\n",
       "==========================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from corsmal_challenge.models.transformer import TransformerEncoder\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batches = 7\n",
    "seq_len = 1000\n",
    "embed_dim = 128\n",
    "num_heads = 8\n",
    "\n",
    "\n",
    "a = torch.randn(batches, seq_len, embed_dim)\n",
    "mha = TransformerEncoder(6, embed_dim, num_heads)\n",
    "a = a.to(device)\n",
    "mha = mha.to(device)\n",
    "\n",
    "\n",
    "b: torch.Tensor = mha(a)\n",
    "print(f\"a.size(): {a.size()}\")\n",
    "print(f\"b.size(): {b.size()}\")\n",
    "\n",
    "torchinfo.summary(mha, inputs_size=(batches, seq_len, embed_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.size(): torch.Size([7, 8, 1000, 128])\n",
      "b.size(): torch.Size([7, 1001, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "LogMelEncoder                                           --\n",
       "├─BatchNorm2d: 1-1                                      16\n",
       "├─DepthWiseConv2d: 1-2                                  416\n",
       "├─PointWiseConv2d: 1-3                                  17\n",
       "├─TransformerEncoder: 1-4                               --\n",
       "│    └─PositionalEncoding: 2-1                          --\n",
       "│    └─MultiheadedSelfAttention: 2-2                    --\n",
       "│    │    └─Linear: 3-1                                 49,536\n",
       "│    │    └─Dropout: 3-2                                --\n",
       "│    │    └─Linear: 3-3                                 16,512\n",
       "│    │    └─Dropout: 3-4                                --\n",
       "│    └─Sequential: 2-3                                  --\n",
       "│    │    └─TransformerEncoderBlock: 3-5                132,224\n",
       "│    │    └─TransformerEncoderBlock: 3-6                132,224\n",
       "│    │    └─TransformerEncoderBlock: 3-7                132,224\n",
       "│    │    └─TransformerEncoderBlock: 3-8                132,224\n",
       "│    │    └─TransformerEncoderBlock: 3-9                132,224\n",
       "================================================================================\n",
       "Total params: 727,617\n",
       "Trainable params: 727,617\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from corsmal_challenge.models.transformer import TransformerEncoder\n",
    "from corsmal_challenge.models.audio import LogMelEncoder\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batches = 7\n",
    "seq_len = 1000\n",
    "in_channels = 8\n",
    "embed_dim = 128\n",
    "num_heads = 4\n",
    "num_layers = 6\n",
    "\n",
    "\n",
    "# a = torch.randn(batches, seq_len, embed_dim)\n",
    "# classifier = TransformerEncoder(num_layers, embed_dim, num_heads)\n",
    "a = torch.randn(batches, in_channels, seq_len, embed_dim)\n",
    "classifier = LogMelEncoder(in_channels, num_layers, embed_dim, num_heads)\n",
    "a = a.to(device)\n",
    "classifier = classifier.to(device)\n",
    "\n",
    "\n",
    "b: torch.Tensor = classifier(a)\n",
    "print(f\"a.size(): {a.size()}\")\n",
    "print(f\"b.size(): {b.size()}\")\n",
    "\n",
    "torchinfo.summary(classifier, inputs_size=a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.size(): torch.Size([1, 8, 500, 128])\n",
      "b.size(): torch.Size([1, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=====================================================================================\n",
       "Layer (type:depth-idx)                                       Param #\n",
       "=====================================================================================\n",
       "TaskChallenger                                               --\n",
       "├─LogMelEncoder: 1-1                                         --\n",
       "│    └─BatchNorm2d: 2-1                                      16\n",
       "│    └─DepthWiseConv2d: 2-2                                  416\n",
       "│    └─PointWiseConv2d: 2-3                                  17\n",
       "│    └─TransformerEncoder: 2-4                               --\n",
       "│    │    └─PositionalEncoding: 3-1                          --\n",
       "│    │    └─MultiheadedSelfAttention: 3-2                    66,048\n",
       "│    │    └─Sequential: 3-3                                  661,120\n",
       "├─T1Head: 1-2                                                --\n",
       "│    └─Linear: 2-5                                           4,128\n",
       "│    └─Linear: 2-6                                           99\n",
       "├─T2Head: 1-3                                                --\n",
       "│    └─Linear: 2-7                                           4,128\n",
       "│    └─Linear: 2-8                                           132\n",
       "=====================================================================================\n",
       "Total params: 736,104\n",
       "Trainable params: 736,104\n",
       "Non-trainable params: 0\n",
       "====================================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchinfo\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from corsmal_challenge.models.audio import LogMelEncoder  # noqa (E402)\n",
    "from corsmal_challenge.models.task1_2 import T1Head, T2Head  # noqa (E402)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batches = 1\n",
    "seq_len = 500\n",
    "in_channels = 8\n",
    "embed_dim = 128\n",
    "num_heads = 4\n",
    "num_layers = 4\n",
    "\n",
    "class TaskChallenger(nn.Module):\n",
    "    def __init__(self, task_id: int = 1):\n",
    "        super(TaskChallenger, self).__init__()\n",
    "        self.task_id = task_id\n",
    "        self.encoder = LogMelEncoder()\n",
    "        self.classify_head1 = T1Head()\n",
    "        self.classify_head2 = T2Head()\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        x: torch.Tensor = self.encoder(inputs)\n",
    "        if self.task_id == 1:\n",
    "            x = self.classify_head1(x[:, 0, :])  # extract embedding of class token\n",
    "        elif self.task_id == 2:\n",
    "            x = self.classify_head2(x[:, 0, :])  # extract embedding of class token\n",
    "        x = x.squeeze(1)\n",
    "        return x\n",
    "\n",
    "# a = torch.randn(batches, seq_len, embed_dim)\n",
    "# model = LiT(num_layers, embed_dim, num_heads)\n",
    "a = torch.randn(batches, in_channels, seq_len, embed_dim)\n",
    "# model = LogMelEncoder(in_channels, num_layers, embed_dim, num_heads)\n",
    "model = TaskChallenger(2).to(device)\n",
    "a = a.to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "b: torch.Tensor = model(a)\n",
    "print(f\"a.size(): {a.size()}\")\n",
    "print(f\"b.size(): {b.size()}\")\n",
    "\n",
    "torchinfo.summary(model, inputs_size=a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.24 ms ± 22.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "model(a)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b459f3be1e56e33a32566beaaadd8e1cfb2ef221d9a730320e6ef9df4350be20"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('miniconda3-4.7.12': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
